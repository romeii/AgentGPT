{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romeii/AgentGPT/blob/main/RAG_Demo_FAISS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Demo with Google Gemini + FAISS\n",
        "\n",
        "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) system that:\n",
        "1. Converts PDF documents to text\n",
        "2. Stores them in FAISS vector database (simple & fast)\n",
        "3. Retrieves relevant context for questions\n",
        "4. Uses Google Gemini to generate answers with citations"
      ],
      "metadata": {
        "id": "notebook_title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "section_install"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai pypdf langchain langchain-community faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configure Google Gemini API"
      ],
      "metadata": {
        "id": "section_config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API key from Colab Secrets\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY_2')\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "config_api"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Gemini Call Function"
      ],
      "metadata": {
        "id": "section_function"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_gemini(question, contexts, model_name=\"models/gemini-2.5-flash\"):\n",
        "    \"\"\"Call Gemini with question and retrieved contexts.\"\"\"\n",
        "    # Build context block\n",
        "    context_block = \"\"\n",
        "    for i, (chunk, source) in enumerate(contexts, start=1):\n",
        "        context_block += f\"[{i}] {chunk}\\n(Source: {source})\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful assistant. Answer ONLY using the provided sources.\n",
        "Cite them inline using [number] format. If the answer is not present, say\n",
        "you don't know.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Sources:\n",
        "{context_block}\n",
        "\"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "gemini_function"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Convert PDFs to Text\n",
        "\n",
        "Upload your PDF files to Colab, then run this cell to convert them to text."
      ],
      "metadata": {
        "id": "section_pdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "# Create docs directory\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "\n",
        "# Convert all PDFs in /content to text\n",
        "pdf_count = 0\n",
        "for filename in os.listdir(\"/content\"):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        pdf_path = f\"/content/{filename}\"\n",
        "        txt_path = f\"/content/docs/{filename}.txt\"\n",
        "\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
        "            reader = PdfReader(pdf_path)\n",
        "            for page in reader.pages:\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    out.write(text + \"\\n\")\n",
        "        pdf_count += 1\n",
        "\n",
        "print(f\"✅ Converted {pdf_count} PDF(s) to text\")\n",
        "print(\"Files in docs/:\", os.listdir(\"/content/docs\"))"
      ],
      "metadata": {
        "id": "pdf_conversion",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "274ef7c6-e6a8-4761-84fe-704c64dcd9e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Converted 4 PDF(s) to text\n",
            "Files in docs/: ['2025-PL-Media-Kit-F.pdf.txt', 'Plate_Social Media Rates.pdf.txt', 'Plate_White Label Marketing Journey.pdf.txt', 'Plate Brand Guidelines 2023.pdf.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load and Chunk Documents"
      ],
      "metadata": {
        "id": "section_load"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Read all text files\n",
        "documents = []\n",
        "for filename in os.listdir(\"/content/docs\"):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        filepath = f\"/content/docs/{filename}\"\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "            documents.append({\"text\": text, \"source\": filename})\n",
        "\n",
        "print(f\"Loaded {len(documents)} document(s)\")\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "chunks = []\n",
        "for doc in documents:\n",
        "    splits = text_splitter.split_text(doc[\"text\"])\n",
        "    for split in splits:\n",
        "        chunks.append({\"text\": split, \"source\": doc[\"source\"]})\n",
        "\n",
        "print(f\"Split into {len(chunks)} chunks\")"
      ],
      "metadata": {
        "id": "load_docs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7595c983-94b3-467e-9de2-3723a222290e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4 document(s)\n",
            "Split into 59 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create FAISS Vector Store"
      ],
      "metadata": {
        "id": "section_faiss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Create embeddings for all chunks\n",
        "print(\"Creating embeddings...\")\n",
        "embeddings = []\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    result = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=chunk[\"text\"],\n",
        "        task_type=\"retrieval_document\"\n",
        "    )\n",
        "    embeddings.append(result['embedding'])\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(chunks)} chunks\")\n",
        "\n",
        "# Convert to numpy array\n",
        "embeddings_array = np.array(embeddings).astype('float32')\n",
        "\n",
        "# Create FAISS index\n",
        "dimension = embeddings_array.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings_array)\n",
        "\n",
        "print(f\"\\n✅ FAISS index created with {index.ntotal} vectors\")"
      ],
      "metadata": {
        "id": "create_faiss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "1acdd342-e53c-49cd-f610-b380a4d9c4b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating embeddings...\n",
            "Processed 10/59 chunks\n",
            "Processed 20/59 chunks\n",
            "Processed 30/59 chunks\n",
            "Processed 40/59 chunks\n",
            "Processed 50/59 chunks\n",
            "\n",
            "✅ FAISS index created with 59 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Query Function"
      ],
      "metadata": {
        "id": "section_query"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_rag(question, k=3):\n",
        "    \"\"\"Query the RAG system with a question.\"\"\"\n",
        "\n",
        "    # Create embedding for the question\n",
        "    question_embedding = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=question,\n",
        "        task_type=\"retrieval_query\"\n",
        "    )['embedding']\n",
        "\n",
        "    # Convert to numpy array\n",
        "    query_vector = np.array([question_embedding]).astype('float32')\n",
        "\n",
        "    # Search FAISS index\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "\n",
        "    # Get the relevant chunks\n",
        "    contexts = []\n",
        "    for idx in indices[0]:\n",
        "        chunk = chunks[idx]\n",
        "        contexts.append((chunk[\"text\"], chunk[\"source\"]))\n",
        "\n",
        "    # Get answer from Gemini\n",
        "    answer = call_gemini(question, contexts)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "query_function"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test Query"
      ],
      "metadata": {
        "id": "section_test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"What are the key topics covered in these documents?\"\n",
        "answer = query_rag(question)\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "test_query"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Interactive Query Cell\n",
        "\n",
        "Use this cell to ask your own questions:"
      ],
      "metadata": {
        "id": "section_interactive"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask your own question\n",
        "my_question = \"What is Plate\"\n",
        "answer = query_rag(my_question)\n",
        "print(f\"Question: {my_question}\\n\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "interactive_query"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}